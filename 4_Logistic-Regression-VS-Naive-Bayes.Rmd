---
title: "Homework 4"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Parker Whitehead"
date: "9/19/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This script will run Logistic Regression and Naive Bayes on the BreastCancer data set which is part of package mlbench. 

## Step 1: Data exploration

* Load package mlbench, installing it at the console if necessary
* Load data(BreastCancer)
* Run str() and head() to look at the data
* Run summary() on the Class column
* Use R code to calculate and output the percentage in each class, with a label using paste()

Comment on the types of predictors available in terms of their data types:
Interestingly, all predictors available are factors. There are numerous factors with 10 levels, and one factor with 9 levels. 

```{r}
# your code here
library(mlbench)
data(BreastCancer)
str(BreastCancer)
head(BreastCancer)
summary(BreastCancer$Class)
percentBenign <- data.frame(table(BreastCancer$Class))[1,2]/nrow(BreastCancer)
percentMalignant <- 1 - percentBenign
print(paste("Percent Benign: ", percentBenign))
print(paste("Percent Malignant: ", percentMalignant))
```

## Step 2: First logistic regression model

* Cell.size and Cell.shape are in one of 10 levels
* Build a logistic regression model called glm0, where Class is predicted by Cell.size and Cell.shape
* Do you get any error or warning messages? Google the message and try to decide what happened
* Run summary on glm0 to confirm that it did build a model
* Write about why you think you got this warning message and what you could possibly do about it.  List the source of your information in a simple markdown link. 

Your commentary here: 

After some research, it seems that the warning message I recieved indicates that the logistic regression model contains too many outlines to be considered valid, and that there is simply not enough data to make any conclusions. 

The predictors we selected seem to have too many possible values and too few datapoints to compensate. We should either create new columns that are binomial factors, dividing the many datapoints into just 2 groups, or seek different data to analyse. 

Source: https://www.statology.org/glm-fit-fitted-probabilities-numerically-0-or-1-occurred/

```{r}
# your code here
glm0 <- glm(Class~Cell.size+Cell.shape, data=BreastCancer, family=binomial)
summary(glm0)
```

## Step 3: Data Wrangling

Notice in the summary() of glm0 that most of the levels of Cell.size and Cell.shape became predictors and that they had very high p-values, that is, they are not good predictors. We would need a lot more data to build a good logistic regression model this way. Many examples per factor level are generally required for model building. A better approach might be to just have 2 levels for each variable. 

In this step:

* Add two new columns to BreastCancer as listed below:
  a.	Cell.small which is a binary factor that is 1 if Cell.size==1 and 0 otherwise
  b.	Cell.regular which is a binary factor that is 1 if Cell.shape==1 and 0 otherwise
* Run summary() on Cell.size and Cell.shape as well as the new columns
* Comment on the distribution of the new columns
* Do you think what we did is a good idea? Why or why not?

Your commentary here:
The distribution of the new columns is consideribly more equally distributed, compared to their 10 factor counterparts.

This was most certainly a good idea, as it will allow us to navigate around the warning we recieved before and acurately record data.

```{r}
# BreastCancer$Cell.small column
BreastCancer$Cell.small <- FALSE
BreastCancer$Cell.small <- ifelse(BreastCancer$Cell.size==1,1,0)
BreastCancer$Cell.small <- factor(BreastCancer$Cell.small)
```

```{r}
# BreastCancer$Cell.regular column
BreastCancer$Cell.regular <- FALSE
BreastCancer$Cell.regular <- ifelse(BreastCancer$Cell.shape==1,1,0)
BreastCancer$Cell.regular <- factor(BreastCancer$Cell.regular)

summary(BreastCancer$Cell.size)
summary(BreastCancer$Cell.shape)
summary(BreastCancer$Cell.small)
summary(BreastCancer$Cell.regular)
```

## Step 4: Examine the relationship of malignancy to Cell.size and Cell.shape

* Create conditional density plots using the original Cell.size and Cell.shape, but first, attach() the data to reduce typing
* Then use par(mfrow=c(1,2)) to set up a 1x2 grid for two cdplot() graphs with Class~Cell.size and Class~Cell.shape
* Observing the plots, write a sentence or two comparing size and malignant, and shape and malignant
* Do you think our cutoff points for size==1 and shape==1 were justified now that you see this graph? Why or why not?

Your commentary here: 
The vast majority of cell sizes that fall under "small" and "regular" are overwhelmingly benign. This displays a very clear display of the potential for Cell.regular and Cell.small as predictors.

```{r}
# your code here
attach(BreastCancer)
par(mfrow=c(1,2))
cdplot(Class~Cell.size)
cdplot(Class~Cell.shape)
```

## Step 5: Explore the new columns

* Create plots (not cdplots) with the two new columns
* Again, use par(mfrow=c(1,2)) to set up a 1x2 grid for two plot() graphs with Class~Cell.small and Class~Cell.regular
* Now create two cdplot() graphs for the new columns
* Compute and output with labels the following: ((Examples on p. 142 may help)
  a.	calculate the percentage of malignant observations that are small 
  b.	calculate the percentage of malignant observations that are not small
  c.	calculate the percentage of malignant observations that are regular
  d.	calculate the percentage of malignant observations that are not regular
* Write whether you think small and regular will be good predictors

Your commentary here:

Small and regular seem to be fantastic predictors if they are indeed small or regular, but waver somewhat if they are False. 

```{r}
# plots here
par(mfrow=c(1,2))
plot(Class~Cell.small)
plot(Class~Cell.regular)

cdplot(Class~Cell.small)
cdplot(Class~Cell.regular)
```

```{r}
# calculations and output here
malignant_small <- table(Cell.small[Class=="malignant"])[[2]]/table(Cell.small[Cell.small==1])[[2]]
print(paste("% Malignant Small Cells: ", malignant_small))

malignant_notsmall <- table(Cell.small[Class=="malignant"])[[1]]/table(Cell.small[Cell.small==0])[[1]]
print(paste("% Malignant Not Small Cells: ", malignant_notsmall))


malignant_regular <- table(Cell.regular[Class=="malignant"])[[2]]/table(Cell.regular[Cell.regular==1])[[2]]
print(paste("% Malignant Regular Cells: ", malignant_regular))

malignant_notregular <- table(Cell.regular[Class=="malignant"])[[1]]/table(Cell.regular[Cell.regular==0])[[1]]
print(paste("% Malignant Not Regular Cells: ", malignant_notregular))
```


## Step 6: Train/test split

* Divide the data into 80/20 train/test sets, using seed 1234


```{r}
# your code here
set.seed(1234)
i <- sample(1:nrow(BreastCancer), nrow(BreastCancer)*.8,replace=FALSE)
train <- BreastCancer[i,]
test <- BreastCancer[-i,]
```


## Step 7: Build a logistic regression model

* Build a logistic regression model predicting malignant with two predictors: Cell.small and Cell. regular
* Run summary() on the model
* Which if any of the predictors are good predictors?
* Comment on the model null variance versus residual variance and what it means
* Comment on the AIC score

Your commentary here:
From glancing at the model, both regular and small seem to be good predictors, although small seems to be somewhat better. 
The residual deviance is considerably less than the null deviance, which is exactly what we want to see.
The AIC is fairly low too, indicating a good model.

```{r}
# your code here
glm1 <- glm(Class~Cell.small+Cell.regular, data=train, family=binomial)
summary(glm1)
```

## Step 8: Evaluate on the test data

* Test the model on the test data 
* Compute and output accuracy 
* Output the confusion matrix and related stats using the confusionMatrix() function in  the caret package
* Were the mis-classifications more false positives or false negatives?

Your commentary here:
There were more false positives than false negatives, if you consider malignant to be a "positive." (Positive case of cancer)

```{r}
# your code here
probs1 <- predict(glm1, newdata=test, type="response")
pred1 <- ifelse(probs1>.5,"malignant","benign")
table(pred1,test$Class)
acc <- mean(pred1==test$Class)
print(paste("Accuracy of model: ",acc))
library(caret)
library(e1071)
pred1 <- factor(pred1)
confusionMatrix(pred1,test$Class)
```

## Step 9: Model coefficients

* The coefficients from the model are in units of logits. Extract and output the coefficient of Cell.small with glm1\$coefficients[]
* Find the estimated probability of malignancy if Cell.small is true using exp(). See the example on p. 107 of the pdf.
* Find the probability of malignancy if Cell.small is true over the whole BreastCancer data set and compare results. Are they close? Why or why not?

Your commentary here:
They are fairly close to each other in comparison to whole percentage, but the probability over the enttire data set is almost double that of the original model. This could be due to outlines that were not captured in the train data.

```{r}
# your code here
small_coef <- glm1$coefficients[[2]]
mal_prob <- exp(small_coef)/(1+exp(small_coef))
print(paste("estimated probability of malignancy if Cell.small is true",mal_prob))

glm2 <- glm(Class~Cell.small+Cell.regular, family=binomial)

small_coef <- glm2$coefficients[[2]]
mal_prob <- exp(small_coef)/(1+exp(small_coef))
print(paste("estimated probability of malignancy if Cell.small is true over entire data set",mal_prob))

```

## Step 10: More logistic regression models

* Build two more models, glm_small using only Cell.small, and glm_regular using Cell.regular as the predictor
* Use anova(glm_small, glm_regular, glm1) to compare all 3 models, using whatever names you used for your models. Analyze the results of the anova(). 
* Also, compare the 3 AIC scores of the models. Feel free to use the internet to help you interpret AIC scores.

Your commentary here:
Both glm_small, regular, and glm1 only deviate from eachother by a small amount, although gml1 deviates from small quite a bit more than regular does.
All AIC scores are well within an acceptable value, although glm1's scores are the lowest. This could simply be a result of a smaller residual deviation, but it is worthy to note, nonetheless. 
```{r}
# your code here
glm_small <- glm(Class~Cell.small, data=train, family=binomial)
glm_regular <- glm(Class~Cell.regular, data=train, family=binomial)
anova(glm_small, glm_regular, glm1)
glm_small$aic
glm_regular$aic
glm1$aic
```

## Step 11: A Naive Bayes model

* Build a Naive Bayes Model Class ~ Cell.small + Cell.regular on the training data using library e1071
* Output the model parameters 
* And answer the following questions:
  a.	What percentage of the training data is benign?
  b.	What is the likelihood that a malignant sample is not small?
  c.	What is the likelihood that a malignant sample is not regular?

Your commentary here:

a) 65.29517% of the train data is benign

b) likelihood = (posterior * marginal) / prior = posterior * predictor / target =( .98969072 * .3470483 ) / 0.450805 = 0.7619048

c) likelihood = 0.98969072 * 0.3470483 / 0.4991055 = 0.6881721 

```{r}
# your code here
nb1 <- naiveBayes(Class~Cell.small+Cell.regular, data=train)
nb1
#Calculating the probability that a cell isn't small/ isn't regular. Used in calculating likelihood.
perc_not_small <- table(train$Cell.small[train$Cell.small==0])[[1]]/sum(table(train$Cell.small))
perc_not_regular <- table(train$Cell.regular[train$Cell.regular==0])[[1]]/sum(table(train$Cell.regular))

perc_not_small
perc_not_regular
```

## Step 12: Evaluate the model

* Predict on the test data with Naive Bayes model
* Output the confusion matrix
* Are the results the same or different? Why do you think that is the case?

Your commentary here:
The results are almost identical. This is most likely due to the fact that Naive Bayes thrives off of smaller data sets. The data set is relatively small and the predictors are very good, so this further contextualizes why the results would be the same/similar.

```{r}
# your code here
probs2 <- predict(nb1, newdata=test,type="class")
confusionMatrix(probs2,test$Class)
```

